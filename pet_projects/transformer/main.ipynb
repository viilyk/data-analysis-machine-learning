{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\n!pip install torchtext","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"wmt17\", \"ru-en\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\", use_fast=True)\n\n\ndef collate_fn(batch):\n    src_texts = [item['translation']['en'] for item in batch]\n    tgt_texts = [item['translation']['ru'] for item in batch]\n\n    src_encodings = tokenizer(src_texts, truncation=True, padding=\"longest\", max_length=128, return_tensors=\"pt\", add_special_tokens=True)\n    tgt_encodings = tokenizer(tgt_texts, truncation=True, padding=\"longest\", max_length=128, return_tensors=\"pt\", add_special_tokens=True)\n\n    input_ids = src_encodings['input_ids']     \n    target_ids = tgt_encodings['input_ids']\n\n    return input_ids, target_ids\n\nprint(tokenizer.vocab_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.eos_token_id)\nprint(tokenizer.bos_token_id)\ntokenizer.special_tokens_map","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport numpy as np\n\ntrain_dataset = dataset[\"train\"].shuffle(seed=42).select(range(500000)) \n\nBATCH_SIZE = 32\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=collate_fn, num_workers=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-(math.log(10000.0) / d_model)))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n\n        seq_len = x.size(0)\n\n        pe = self.pe[:seq_len].unsqueeze(1)\n        return x + pe.to(x.device)\n\n\nclass TranslationTransformer(nn.Module):\n    def __init__(self, vocab_size, d_model=256, nhead=8, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=512, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.src_embedding = nn.Embedding(vocab_size, d_model, padding_idx=tokenizer.pad_token_id)\n        self.tgt_embedding = nn.Embedding(vocab_size, d_model, padding_idx=tokenizer.pad_token_id)\n        self.positional_encoding = PositionalEncoding(d_model)\n        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n                                          num_encoder_layers=num_encoder_layers,\n                                          num_decoder_layers=num_decoder_layers,\n                                          dim_feedforward=dim_feedforward,\n                                          dropout=dropout,\n                                          batch_first=False\n                                         )\n        self.fc_out = nn.Linear(d_model, vocab_size)\n\n    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n        src_emb = self.src_embedding(src) * math.sqrt(self.d_model) \n        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n\n        src_emb = self.positional_encoding(src_emb)\n        tgt_emb = self.positional_encoding(tgt_emb)\n\n        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_emb.size(0)).to(src.device)\n\n        out = self.transformer(src_emb, tgt_emb,\n                               tgt_mask=tgt_mask,\n                               src_key_padding_mask=src_key_padding_mask,\n                               tgt_key_padding_mask=tgt_key_padding_mask,\n                               memory_key_padding_mask=memory_key_padding_mask)\n        return self.fc_out(out)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\nfrom tqdm import tqdm\n\nvocab_size = tokenizer.vocab_size\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = TranslationTransformer(vocab_size=vocab_size).to(device)\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\noptimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n\ntrain_losses = []\nvalid_losses = []\nfrom torch.cuda.amp import GradScaler, autocast\n\nscaler = GradScaler() if torch.cuda.is_available() else None\n\ndef make_padding_mask(seq):\n    # seq: (seq_len, batch)\n    return (seq == tokenizer.pad_token_id).transpose(0, 1)  # (batch, seq_len)\n\nfrom torch.optim.lr_scheduler import LambdaLR\n\n# def get_inverse_sqrt_scheduler(optimizer, d_model=256, warmup_steps=4000, scale=1e-3):\n#     def lr_lambda(step):\n#         step += 1\n#         return (d_model ** -0.5) * min(step ** -0.5, step * warmup_steps ** -1.5) * scale \n#     return LambdaLR(optimizer, lr_lambda)\n\n# warmup_steps = 2000\n# scheduler = get_inverse_sqrt_scheduler(optimizer, warmup_steps)\n\ndef train_epoch(model, dataloader, optimizer, criterion, device, scheduler=None, grad_clip=1.0):\n    model.train()\n    total_loss = 0\n    for src_batch, tgt_batch in tqdm(dataloader, desc=\"Training\"):\n        print(tokenizer.convert_ids_to_tokens(tgt_batch[0]))\n        # src_batch, tgt_batch: (batch, seq_len)\n        src = src_batch.transpose(0,1).to(device)   # (src_len, batch)\n        tgt = tgt_batch.transpose(0,1).to(device)   # (tgt_len, batch)\n\n        # prepare decoder input: decoder sees all tokens except last\n        tgt_input = tgt[:-1, :]   # (tgt_len-1, batch)\n        tgt_expected = tgt[1:, :] # (tgt_len-1, batch)\n\n        src_key_padding_mask = make_padding_mask(src).to(device)  # (batch, src_len)\n        tgt_key_padding_mask = make_padding_mask(tgt_input).to(device)  # (batch, tgt_len-1)\n\n        optimizer.zero_grad()\n        if scaler is not None:\n            with autocast():\n                output = model(src, tgt_input, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n                # output: (tgt_len-1, batch, vocab)\n                loss = criterion(output.view(-1, output.size(-1)), tgt_expected.contiguous().view(-1))\n            scaler.scale(loss).backward()\n            # if grad_clip is not None:\n            #     scaler.unscale_(optimizer)\n            #     torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            output = model(src, tgt_input, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n            loss = criterion(output.view(-1, output.size(-1)), tgt_expected.contiguous().view(-1))\n            loss.backward()\n            # if grad_clip is not None:\n            #     torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            optimizer.step()\n            \n        # if scheduler is not None:\n        #     scheduler.step()\n\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses = []\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    loss = train_epoch(model, train_dataloader, optimizer, criterion, device)\n    train_losses.append(loss)\n    print(f\"Epoch {epoch+1}: Train Loss={loss:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(range(1, num_epochs+1), train_losses, marker=\"o\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Train Loss\")\nplt.title(\"Training Loss\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/translation_transformer.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def greedy_decode_batch(model, src, tokenizer, device, max_len=100):\n    model.eval()\n    with torch.no_grad():\n        batch_size = src.size(0)\n        src = src.transpose(0, 1).to(device)  # (src_len, batch)\n        src_key_padding_mask = (src.transpose(0,1) == tokenizer.pad_token_id).to(device)  # (batch, src_len)\n\n        bos = tokenizer.bos_token_id or tokenizer.cls_token_id or tokenizer.pad_token_id\n        eos = tokenizer.eos_token_id or tokenizer.sep_token_id or tokenizer.pad_token_id\n\n        generated = torch.full((1, batch_size), bos, dtype=torch.long, device=device)  # (1, batch)\n\n        finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n\n        for _ in range(max_len):\n            tgt_mask = model.transformer.generate_square_subsequent_mask(generated.size(0)).to(device)\n            tgt_key_padding_mask = (generated.transpose(0,1) == tokenizer.pad_token_id)  # (batch, seq_len)\n\n            out = model(src, generated,\n                        src_key_padding_mask=src_key_padding_mask,\n                        tgt_key_padding_mask=tgt_key_padding_mask)\n\n            next_tokens = out[-1].argmax(dim=-1)  # (batch,)\n            generated = torch.cat([generated, next_tokens.unsqueeze(0)], dim=0)\n\n            finished |= (next_tokens == eos)\n            if finished.all():\n                break\n\n        results = []\n        generated = generated.transpose(0,1).cpu().tolist()  # (batch, seq_len)\n        for seq in generated:\n            results.append(tokenizer.decode(seq, skip_special_tokens=True))\n        return results\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sacrebleu\n\ndef test_model(model, dataloader, tokenizer, device, max_len=100):\n    all_hypotheses = []\n    all_references = []\n\n    for src_batch, tgt_batch in tqdm(dataloader, desc=\"Testing\"):\n        hypotheses = greedy_decode_batch(model, src_batch, tokenizer, device, max_len=max_len)\n\n        for ref_ids in tgt_batch.tolist():\n            all_references.append(tokenizer.decode(ref_ids, skip_special_tokens=True))\n        all_hypotheses.extend(hypotheses)\n\n    bleu = sacrebleu.corpus_bleu(all_hypotheses, [all_references])\n\n    return bleu.score, all_references[:5], all_hypotheses[:5]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = dataset['test']\ntest_dataloader = DataLoader(test_dataset, batch_size=1,\n                              shuffle=True, collate_fn=collate_fn, num_workers=4)\n\nbleu, refs_sample, hyps_sample = test_model(model, test_dataloader, tokenizer, device, max_len=60)\nprint(f\"\\nFinal BLEU score on test set: {bleu:.2f}\")\n\nprint(\"\\nПримеры перевода:\")\nfor r, h in zip(refs_sample[:5], hyps_sample[:5]):\n    print(f\"REF: {r}\")\n    print(f\"HYP: {h}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not model:\n  model = TranslationTramsformer(tokenizer.vocab_size)\n  model.load_state_dict(torch.load('/content/my_model.pth'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate_sentence(model, tokenizer, sentence, device, max_len=50):\n    model.eval()\n\n    src_enc = tokenizer(\n        [sentence],\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=\"longest\",\n        max_length=128\n    ).to(device)\n\n    src_ids = src_enc[\"input_ids\"].transpose(0, 1)  # [seq_len, batch]\n    src_pad_mask = (src_ids == tokenizer.pad_token_id).transpose(0, 1).to(device)\n\n    start_token = tokenizer.bos_token_id or tokenizer.cls_token_id or tokenizer.pad_token_id\n    eos_token = tokenizer.eos_token_id or tokenizer.sep_token_id or tokenizer.pad_token_id\n\n    generated_tokens = [start_token]\n    for _ in range(max_len):\n        tgt_input = torch.tensor(generated_tokens, dtype=torch.long, device=device).unsqueeze(1)\n        tgt_pad_mask = (tgt_input == tokenizer.pad_token_id).transpose(0, 1)\n\n        with torch.no_grad():\n            output = model(src_ids, tgt_input, src_pad_mask, tgt_pad_mask)\n\n        next_token = output[-1, 0].argmax().item()\n        if next_token == eos_token:\n            break\n        generated_tokens.append(next_token)\n\n    translation = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n    return translation\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_sentences = [\n    \"Hello, how are you?\",\n    \"The transformer model is very powerful for sequence-to-sequence tasks.\",\n    \"In 2017, researchers proposed the architecture known as 'Attention Is All You Need'.\",\n    \"Where are you from?\"\n]\n\nfor s in example_sentences:\n    print(\"EN:\", s)\n    print(\"RU:\", translate_sentence(model, tokenizer, s, device))\n    print()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}