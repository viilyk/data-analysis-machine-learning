{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13006899,"sourceType":"datasetVersion","datasetId":8234485}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a2d7ca3b3f826da4","cell_type":"code","source":"import cv2\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nclass OCRDataset(Dataset):\n    def __init__(self, img_paths: List[str], labels: List[str], transform=None):\n        assert len(img_paths) == len(labels)\n        self.img_paths = img_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.img_paths[idx]).convert(\"L\")\n        if self.transform:\n            img = self.transform(img)\n        label = self.labels[idx]\n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:36:47.706121Z","iopub.execute_input":"2025-09-08T15:36:47.706781Z","iopub.status.idle":"2025-09-08T15:36:47.879253Z","shell.execute_reply.started":"2025-09-08T15:36:47.706747Z","shell.execute_reply":"2025-09-08T15:36:47.878501Z"}},"outputs":[],"execution_count":2},{"id":"ce3d4d47-ec4b-4233-835d-a20f262ecab9","cell_type":"code","source":"TRAIN_ROOT = \"/kaggle/input/sintetic-texts/ocr_dataset/train\"  \nTEST_ROOT  = \"/kaggle/input/sintetic-texts-multibackground/ocr_dataset/test\"   \nOUT_DIR    = \"/kaggele/output/checkpoints\"\n\nos.makedirs(OUT_DIR, exist_ok=True)\n\nall_labels = train_labels + val_labels + test_labels\nchar_to_idx, idx_to_char = build_charset(all_labels)\nnum_classes = max(char_to_idx.values()) + 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cb3ac2a7-f090-4cf2-a382-ef9fa6591e49","cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef read_labels_file(label_file: str, img_dir: str) -> Tuple[List[str], List[str]]:\n    img_paths, labels = [], []\n    with open(label_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            parts = line.split(maxsplit=1)\n            if len(parts) == 1:\n                fname, text = parts[0], \"\"\n            else:\n                fname, text = parts\n            p = os.path.join(img_dir, fname)\n            if os.path.exists(p):\n                img_paths.append(p)\n                labels.append(text)\n    return img_paths, labels\n\n\ndef load_datasets(root: str = TRAIN_ROOT, test_root: str = TEST_ROOT,\n                  val_split: float = 0.1, seed: int = 42):\n    random.seed(seed)\n    train_paths, train_labels = [], []\n    val_paths, val_labels = [], []\n    train_levels = []  # parallel list of 'A'/'B'/'C' for curriculum\n\n    for subset in [\"A\", \"B\", \"C\"]:\n        dir_subset = os.path.join(root, subset)\n        img_dir = os.path.join(dir_subset, \"images\")\n        label_file = os.path.join(dir_subset, \"labels.txt\")\n        imgs, labs = read_labels_file(label_file, img_dir)\n\n        # shuffle inside subset to avoid ordering by background complexity\n        combined = list(zip(imgs, labs))\n        random.shuffle(combined)\n        imgs, labs = zip(*combined) if combined else ([], [])\n\n        n_val = int(len(imgs) * val_split)\n        if n_val > 0:\n            val_paths.extend(imgs[:n_val])\n            val_labels.extend(labs[:n_val])\n\n        train_paths.extend(imgs[n_val:])\n        train_labels.extend(labs[n_val:])\n        train_levels.extend([subset] * (len(imgs) - n_val))\n\n    # test\n    test_img_dir = os.path.join(test_root, \"images\")\n    test_label_file = os.path.join(test_root, \"labels.txt\")\n    test_paths, test_labels = read_labels_file(test_label_file, test_img_dir)\n\n    return (train_paths, train_labels, train_levels), (val_paths, val_labels), (test_paths, test_labels)\n\n\n(train_paths, train_labels, train_levels), (val_paths, val_labels), (test_paths, test_labels) = load_datasets()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"177d8f64-6f5a-484d-97f4-640e568cc0f0","cell_type":"code","source":"def build_charset(all_labels: List[str], extra_chars: str = \"\"):\n    chars = set()\n    for s in all_labels:\n        chars.update(list(s))\n    chars.update(list(extra_chars))\n    chars = sorted(chars)\n    # blank = 0\n    char_to_idx = {c: i + 1 for i, c in enumerate(chars)}  # start from 1\n    idx_to_char = {i + 1: c for i, c in enumerate(chars)}\n    # blank represented by 0\n    return char_to_idx, idx_to_char","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6432fff6-099d-40dc-a2b9-95cbc2a39bcc","cell_type":"code","source":"def ctc_collate(batch, char_to_idx):\n    imgs, labels = zip(*batch)\n    imgs = torch.stack(imgs, dim=0)\n\n    targets = []\n    target_lengths = []\n    for lab in labels:\n        idxs = []\n        for ch in lab:\n            # unknown char -> skip (could also map to some special token)\n            if ch in char_to_idx:\n                idxs.append(char_to_idx[ch])\n        targets.extend(idxs)\n        target_lengths.append(len(idxs))\n\n    if len(targets) == 0:\n        targets_tensor = torch.zeros(0, dtype=torch.long)\n    else:\n        targets_tensor = torch.tensor(targets, dtype=torch.long)\n\n    target_lengths = torch.tensor(target_lengths, dtype=torch.long)\n    return imgs, targets_tensor, target_lengths, list(labels)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"35f5b391-2c8d-4cba-a204-1e59c8142ca4","cell_type":"code","source":"def greedy_ctc_decode(logits, idx_to_char, blank_index=0):\n    with torch.no_grad():\n        # choose most probable class at each time step\n        preds = torch.argmax(logits, dim=2)  # [T, B]\n        preds = preds.transpose(0, 1).cpu().numpy()  # [B, T]\n    decoded = []\n    for seq in preds:\n        last = None\n        out_chars = []\n        for p in seq:\n            if p == last:\n                continue\n            last = p\n            if p != blank_index:\n                ch = idx_to_char.get(int(p), \"\")\n                out_chars.append(ch)\n        decoded.append(\"\".join(out_chars))\n    return decoded\n\n\ndef edit_distance(a: str, b: str) -> int:\n    la, lb = len(a), len(b)\n    if la == 0: return lb\n    if lb == 0: return la\n    dp = [[0] * (lb + 1) for _ in range(la + 1)]\n    for i in range(la + 1):\n        dp[i][0] = i\n    for j in range(lb + 1):\n        dp[0][j] = j\n    for i in range(1, la + 1):\n        for j in range(1, lb + 1):\n            cost = 0 if a[i - 1] == b[j - 1] else 1\n            dp[i][j] = min(dp[i - 1][j] + 1,      # del\n                           dp[i][j - 1] + 1,      # ins\n                           dp[i - 1][j - 1] + cost)  # sub\n    return dp[la][lb]\n\n\ndef cer_batch(preds: List[str], targets: List[str]) -> float:\n    total_ed = 0\n    total_chars = 0\n    for p, t in zip(preds, targets):\n        total_ed += edit_distance(p, t)\n        total_chars += max(1, len(t))\n    return total_ed / total_chars\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5be83dedce9759cd","cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CRNN(nn.Module):\n    def __init__(self, img_h=32, num_channels=1, num_classes=37, hidden_size=128):\n        super(CRNN, self).__init__()\n        \n        # CNN \n        self.cnn = nn.Sequential(\n            nn.Conv2d(num_channels, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2),    \n\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2), \n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d((2, 1), (2, 1)), \n\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n        )\n        \n        # BiLSTM\n        self.rnn = nn.LSTM(\n            input_size=256*4,\n            hidden_size=hidden_size,\n            num_layers=2,\n            bidirectional=True,\n            batch_first=True\n        )\n        \n        # Классификатор\n        self.fc = nn.Linear(hidden_size*2, num_classes)\n\n    def forward(self, x):\n        # x: [B, 1, H, W]\n        conv = self.cnn(x)        \n        b, c, h, w = conv.size()\n        \n        conv = conv.permute(0, 3, 1, 2) \n        conv = conv.reshape(b, w, c*h) \n        \n        rnn_out, _ = self.rnn(conv)  \n        logits = self.fc(rnn_out)\n        \n        logits = logits.permute(1, 0, 2) \n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:39:12.853226Z","iopub.execute_input":"2025-09-08T15:39:12.853734Z","iopub.status.idle":"2025-09-08T15:39:12.860142Z","shell.execute_reply.started":"2025-09-08T15:39:12.853694Z","shell.execute_reply":"2025-09-08T15:39:12.859458Z"}},"outputs":[],"execution_count":12},{"id":"ce3aa299-ba7f-473d-a22e-169ce4f861d5","cell_type":"code","source":"def make_curriculum_weights(level_labels: List[str], epoch: int, max_epoch: int):\n    alpha = min(epoch / max_epoch, 1.0)\n    weights_map = {\n        \"A\": max(0.05, 1.0 - 0.8 * alpha), \n        \"B\": 0.2 + 0.4 * alpha,\n        \"C\": 0.1 + 0.6 * alpha\n    }\n    return [weights_map.get(lvl, 0.1) for lvl in level_labels]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"16f9a2d1-c0e6-4d24-8ec9-11c52f68062a","cell_type":"code","source":"def train_loop(\n    model: nn.Module,\n    device: torch.device,\n    train_ds_tuple,\n    val_ds_tuple,\n    char_to_idx,\n    idx_to_char,\n    batch_size: int = 32,\n    num_epochs: int = 30,\n    lr: float = 1e-3,\n    max_epoch_for_curriculum: int = 20,\n    save_best_path: str = os.path.join(OUT_DIR, \"best.pth\"),\n):\n    (train_paths, train_labels, train_levels) = train_ds_tuple\n    (val_paths, val_labels) = val_ds_tuple\n\n    # transforms (same as before)\n    transform = T.Compose([\n        T.Resize((32, 128)),\n        T.ToTensor(),\n        T.Normalize((0.5,), (0.5,))\n    ])\n\n    train_ds = OCRDataset(train_paths, train_labels, transform)\n    val_ds = OCRDataset(val_paths, val_labels, transform)\n\n    # optimizer, loss\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True).to(device)\n\n    best_val_loss = float(\"inf\")\n\n    for epoch in range(1, num_epochs + 1):\n        # --- build curriculum sampler for this epoch ---\n        weights = make_curriculum_weights(train_levels, epoch, max_epoch_for_curriculum)\n        sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n        train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler,\n                                  collate_fn=lambda b: ctc_collate(b, char_to_idx),\n                                  num_workers=4, pin_memory=True)\n\n        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n                                collate_fn=lambda b: ctc_collate(b, char_to_idx),\n                                num_workers=2, pin_memory=True)\n\n        # --- train epoch ---\n        model.train()\n        total_train_loss = 0.0\n        n_train_batches = 0\n        for imgs, targets, target_lengths, raw_labels in train_loader:\n            imgs = imgs.to(device)                       \n            targets = targets.to(device)\n            target_lengths = target_lengths.to(device)\n\n            optimizer.zero_grad()\n            logits = model(imgs)  \n            T_time, B_batch, C_classes = logits.size()\n            input_lengths = torch.full((B_batch,), T_time, dtype=torch.long).to(device)\n\n            log_probs = logits.log_softmax(2)  # along class dim\n\n            loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n            optimizer.step()\n\n            total_train_loss += loss.item()\n            n_train_batches += 1\n\n        avg_train_loss = total_train_loss / max(1, n_train_batches)\n\n        # validation \n        model.eval()\n        total_val_loss = 0.0\n        n_val_batches = 0\n        total_cer = 0.0\n        with torch.no_grad():\n            for imgs, targets, target_lengths, raw_labels in val_loader:\n                imgs = imgs.to(device)\n                targets = targets.to(device)\n                target_lengths = target_lengths.to(device)\n\n                logits = model(imgs)\n                T_time, B_batch, C_classes = logits.size()\n                input_lengths = torch.full((B_batch,), T_time, dtype=torch.long).to(device)\n                log_probs = logits.log_softmax(2)\n\n                loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n                total_val_loss += loss.item()\n                n_val_batches += 1\n\n                # decode predictions and compute CER\n                # use greedy decoding\n                decoded = greedy_ctc_decode(logits, idx_to_char, blank_index=0)\n                # raw_labels are original strings\n                total_cer += cer_batch(decoded, raw_labels)\n\n        avg_val_loss = total_val_loss / max(1, n_val_batches)\n        avg_cer = total_cer / max(1, n_val_batches)\n\n        print(f\"Epoch {epoch}/{num_epochs} | train_loss: {avg_train_loss:.4f} | val_loss: {avg_val_loss:.4f} | val_CER: {avg_cer:.4f}\")\n\n        # save best\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save({\n                \"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n                \"char_to_idx\": char_to_idx,\n                \"idx_to_char\": idx_to_char,\n                \"val_loss\": avg_val_loss\n            }, save_best_path)\n            print(f\"Saved best model at epoch {epoch} with val_loss {avg_val_loss:.4f}\")\n\n    print(\"Training finished.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a6e9cc33-7411-43a7-9d52-fb2ce9c03a55","cell_type":"code","source":"model = CRNN(img_h=32, num_channels=1, num_classes=num_classes, hidden_size=128)\nmodel = model.to(device)\n\ntrain_loop(\n    model=model,\n    device=device,\n    train_ds_tuple=(train_paths, train_labels, train_levels),\n    val_ds_tuple=(val_paths, val_labels),\n    char_to_idx=char_to_idx,\n    idx_to_char=idx_to_char,\n    batch_size=32,\n    num_epochs=30,\n    lr=1e-3,\n    max_epoch_for_curriculum=20,\n    save_best_path=os.path.join(OUT_DIR, \"best_crnn.pth\")\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}